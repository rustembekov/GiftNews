import aiohttp
import asyncio
import feedparser
from typing import List, Dict, Any, Optional
import json
from datetime import datetime, timedelta
import logging
import re
import hashlib

logger = logging.getLogger(__name__)


class TelegramNewsService:
    """–°–µ—Ä–≤–∏—Å –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –Ω–æ–≤–æ—Å—Ç–µ–π –∏–∑ Telegram –∫–∞–Ω–∞–ª–æ–≤ –∏ RSS –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤"""

    def __init__(self):
        # –¢–æ–ª—å–∫–æ –∫–∞–Ω–∞–ª @nextgen_NFT –ø–æ –∑–∞–ø—Ä–æ—Å—É –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        self.channels = [
            {'username': 'nextgen_NFT', 'name': 'NextGen NFT', 'category': 'nft'}
        ]

        # RSS –∏—Å—Ç–æ—á–Ω–∏–∫–∏ —Å–æ–≥–ª–∞—Å–Ω–æ –¢–ó - –¥–æ 5 –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã—Ö –ª–µ–Ω—Ç
        self.rss_sources = [
            {'url': 'https://vc.ru/rss', 'name': 'VC.ru', 'category': 'tech'},
            {'url': 'https://www.coindesk.com/arc/outboundfeeds/rss/', 'name': 'CoinDesk', 'category': 'crypto'},
            {'url': 'https://cointelegraph.com/rss', 'name': 'Cointelegraph', 'category': 'crypto'},
            {'url': 'https://habr.com/ru/rss/articles/', 'name': 'Habr NFT', 'category': 'nft'}
        ]

        self.cache = {}
        self.cache_ttl = timedelta(minutes=30)  # –ö—ç—à –Ω–∞ 30 –º–∏–Ω—É—Ç —Å–æ–≥–ª–∞—Å–Ω–æ –¢–ó

        # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏–∏ —Å–æ–≥–ª–∞—Å–Ω–æ –¢–ó
        self.keywords = {
            'gifts': [
                '–ø–æ–¥–∞—Ä–æ–∫', '–ø–æ–¥–∞—Ä–∫–∏', '–±–µ—Å–ø–ª–∞—Ç–Ω–æ', '—Ö–∞–ª—è–≤–∞', '–ø—Ä–æ–º–æ–∫–æ–¥', '—Å–∫–∏–¥–∫–∞',
                '–∞–∫—Ü–∏—è', '—Ä–æ–∑—ã–≥—Ä—ã—à', '–±–æ–Ω—É—Å', '–¥–∞—Ä–æ–º', '–≥–∏—Ñ—Ç', 'gift', 'freebie',
                '—Ä–∞–∑–¥–∞—á–∞', '–∫–æ–Ω–∫—É—Ä—Å', '–ø—Ä–∏–∑', '–Ω–∞–≥—Ä–∞–¥–∞', 'cashback', '–∫—ç—à–±–µ–∫'
            ],
            'nft': [
                'nft', '–Ω—Ñ—Ç', '—Ç–æ–∫–µ–Ω', '–∫–æ–ª–ª–µ–∫—Ü–∏—è', '–º–µ—Ç–∞', 'opensea', 'digital art',
                '–∫–æ–ª–ª–µ–∫—Ü–∏–æ–Ω–Ω—ã–π', '—Ü–∏—Ñ—Ä–æ–≤–æ–µ –∏—Å–∫—É—Å—Å—Ç–≤–æ', '–º–µ—Ç–∞–≤—Å–µ–ª–µ–Ω–Ω–∞—è', 'avatar',
                '–∞–≤–∞—Ç–∞—Ä', 'pfp', 'mint', '–º–∏–Ω—Ç', 'drop', '–¥—Ä–æ–ø', 'rare', '—Ä–∞—Ä–∏—Ç–µ—Ç'
            ],
            'crypto': [
                '–∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç–∞', '–±–∏—Ç–∫–æ–∏–Ω', 'bitcoin', 'ethereum', '–±–ª–æ–∫—á–µ–π–Ω', '–¥–µ—Ñ',
                'defi', '—Ç–æ—Ä–≥–∏', '–∫—É—Ä—Å', 'btc', 'eth', 'usdt', 'binance', '—Ç—Ä–µ–π–¥–∏–Ω–≥',
                '—Å—Ç–µ–π–∫–∏–Ω–≥', '–º–∞–π–Ω–∏–Ω–≥', 'altcoin', '–∞–ª—å—Ç–∫–æ–∏–Ω', 'pump', 'dump', 'hodl'
            ],
            'tech': [
                '—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏', 'it', '–∏—Ç', '–ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ', '—Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞', '—Å—Ç–∞—Ä—Ç–∞–ø',
                '–∏–Ω–Ω–æ–≤–∞—Ü–∏–∏', 'ai', '–∏–∏', 'machine learning', '–±–ª–æ–∫—á–µ–π–Ω', '–≤–µ–±3',
                'app', '–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ', 'software', 'hardware', 'gadget', '–≥–∞–¥–∂–µ—Ç'
            ],
            'community': [
                '—Å–æ–æ–±—â–µ—Å—Ç–≤–æ', '—á–∞—Ç', '–æ–±—â–µ–Ω–∏–µ', '—Ñ–æ—Ä—É–º', '–¥–∏—Å–∫—É—Å—Å–∏—è', '–º–Ω–µ–Ω–∏–µ',
                '–æ–±—Å—É–∂–¥–µ–Ω–∏–µ', '–Ω–æ–≤–æ—Å—Ç–∏', '–∞–Ω–æ–Ω—Å', '–≤—Å—Ç—Ä–µ—á–∞', 'event', '–º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏–µ'
            ]
        }

    def categorize_content(self, title: str, description: str = "") -> str:
        """
        –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º —Å–æ–≥–ª–∞—Å–Ω–æ –¢–ó
        –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç: gifts > crypto > nft > tech > community
        """
        content = (title + " " + description).lower()

        # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –¥–ª—è –∫–∞–∂–¥–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
        category_scores = {}
        for category, keywords in self.keywords.items():
            score = sum(1 for keyword in keywords if keyword in content)
            if score > 0:
                category_scores[category] = score

        if not category_scores:
            return 'general'

        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏—é —Å –Ω–∞–∏–±–æ–ª—å—à–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π
        # –ü—Ä–∏ —Ä–∞–≤–µ–Ω—Å—Ç–≤–µ –æ—á–∫–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç
        priority = ['gifts', 'crypto', 'nft', 'tech', 'community']

        max_score = max(category_scores.values())
        best_categories = [cat for cat, score in category_scores.items() if score == max_score]

        for priority_cat in priority:
            if priority_cat in best_categories:
                return priority_cat

        return list(category_scores.keys())[0]  # Fallback

    async def fetch_telegram_channel(self, channel_username: str) -> List[Dict[str, Any]]:
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–µ–π –∏–∑ Telegram –∫–∞–Ω–∞–ª–∞ —á–µ—Ä–µ–∑ –≤–µ–±-—Å–∫—Ä–∞–ø–∏–Ω–≥
        –°–æ–≥–ª–∞—Å–Ω–æ –¢–ó - –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å Telegram –∫–∞–Ω–∞–ª–∞–º–∏ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –∞–∫—Ç—É–∞–ª—å–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π
        """
        try:
            channel_data = next((ch for ch in self.channels if ch['username'] == channel_username), None)
            if not channel_data:
                logger.warning(f"Channel {channel_username} not found in configured channels")
                return []

            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—É–±–ª–∏—á–Ω—ã–π API Telegram –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –ø–æ—Å—Ç–æ–≤
            url = f"https://t.me/s/{channel_username}"

            async with aiohttp.ClientSession() as session:
                try:
                    async with session.get(url, timeout=10) as response:
                        if response.status == 200:
                            html_content = await response.text()
                            return self._parse_telegram_html(html_content, channel_data)
                        else:
                            logger.warning(f"Failed to fetch {url}, status: {response.status}")
                            return self._generate_mock_posts(channel_data)
                except aiohttp.ClientTimeout:
                    logger.warning(f"Timeout fetching {url}, using mock data")
                    return self._generate_mock_posts(channel_data)
                except Exception as e:
                    logger.warning(f"Error fetching {url}: {e}, using mock data")
                    return self._generate_mock_posts(channel_data)

        except Exception as e:
            logger.error(f"Error in fetch_telegram_channel for {channel_username}: {e}")
            return []

    def _parse_telegram_html(self, html_content: str, channel_data: Dict) -> List[Dict[str, Any]]:
        """–ü–∞—Ä—Å–∏–Ω–≥ HTML —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ Telegram –∫–∞–Ω–∞–ª–∞ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –º–µ–¥–∏–∞ –∏ –ø–æ–ª–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
        import re
        from html import unescape
        from bs4 import BeautifulSoup

        posts = []

        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º BeautifulSoup –¥–ª—è –±–æ–ª–µ–µ —Ç–æ—á–Ω–æ–≥–æ –ø–∞—Ä—Å–∏–Ω–≥–∞
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # –ù–∞—Ö–æ–¥–∏–º –≤—Å–µ —Å–æ–æ–±—â–µ–Ω–∏—è
            message_widgets = soup.find_all('div', class_='tgme_widget_message')
            
            for i, message in enumerate(message_widgets[:15]):  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –ª–∏–º–∏—Ç –¥–æ 15 –ø–æ—Å—Ç–æ–≤
                try:
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç —Å–æ–æ–±—â–µ–Ω–∏—è
                    text_widget = message.find('div', class_='tgme_widget_message_text')
                    if not text_widget:
                        continue
                    
                    # –ü–æ–ª—É—á–∞–µ–º –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç –±–µ–∑ –æ–±—Ä–µ–∑–∫–∏
                    full_text = text_widget.get_text(strip=True)
                    if not full_text:
                        continue
                    
                    # –û—á–∏—â–∞–µ–º —Ç–µ–∫—Å—Ç –æ—Ç –ª–∏—à–Ω–∏—Ö –ø—Ä–æ–±–µ–ª–æ–≤
                    full_text = re.sub(r'\s+', ' ', full_text).strip()
                    
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º HTML –∫–æ–Ω—Ç–µ–Ω—Ç –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
                    html_content = str(text_widget)
                    
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞—Ç—É
                    time_element = message.find('time')
                    date = datetime.now().isoformat()
                    if time_element and time_element.get('datetime'):
                        try:
                            date_str = time_element['datetime']
                            date = datetime.fromisoformat(date_str.replace('Z', '+00:00')).isoformat()
                        except:
                            pass
                    
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º –º–µ–¥–∏–∞ –∫–æ–Ω—Ç–µ–Ω—Ç
                    media = None
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ —Ñ–æ—Ç–æ - —É–ª—É—á—à–µ–Ω–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥
                    photo_wrap = message.find('a', class_='tgme_widget_message_photo_wrap')
                    if photo_wrap:
                        # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ —Å–ø–æ—Å–æ–±—ã –∏–∑–≤–ª–µ—á–µ–Ω–∏—è URL
                        photo_url = None
                        
                        # –°–ø–æ—Å–æ–± 1: —á–µ—Ä–µ–∑ style background-image
                        style = photo_wrap.get('style', '')
                        photo_url_match = re.search(r'background-image:url\(&quot;([^&]+)&quot;\)', style)
                        if photo_url_match:
                            photo_url = photo_url_match.group(1).replace('&amp;', '&')
                        
                        # –°–ø–æ—Å–æ–± 2: —á–µ—Ä–µ–∑ href –∞—Ç—Ä–∏–±—É—Ç
                        if not photo_url:
                            photo_url = photo_wrap.get('href')
                            if photo_url and photo_url.startswith('//'):
                                photo_url = 'https:' + photo_url
                        
                        # –°–ø–æ—Å–æ–± 3: —á–µ—Ä–µ–∑ img –≤–Ω—É—Ç—Ä–∏
                        if not photo_url:
                            img_element = photo_wrap.find('img')
                            if img_element:
                                photo_url = img_element.get('src')
                                if photo_url and photo_url.startswith('//'):
                                    photo_url = 'https:' + photo_url
                        
                        if photo_url:
                            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä—è–º—ã–µ —Å—Å—ã–ª–∫–∏ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è Telegram
                            if photo_url.startswith('https://t.me/'):
                                # –ò–∑–≤–ª–µ–∫–∞–µ–º ID —Å–æ–æ–±—â–µ–Ω–∏—è –∏ —Å–æ–∑–¥–∞–µ–º –ø—Ä—è–º—É—é —Å—Å—ã–ª–∫—É
                                msg_id_match = re.search(r'/(\d+)(?:\?.*)?$', photo_url)
                                if msg_id_match:
                                    msg_id = msg_id_match.group(1)
                                    # –°–æ–∑–¥–∞–µ–º –ø—Ä—è–º—É—é —Å—Å—ã–ª–∫—É –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
                                    photo_url = f"https://t.me/nextgen_NFT/{msg_id}?single"
                            
                            media = {
                                'type': 'photo',
                                'url': photo_url,
                                'thumbnail': photo_url,
                                'width': None,
                                'height': None
                            }
                            logger.info(f"Found photo: {photo_url}")
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –≤–∏–¥–µ–æ - —É–ª—É—á—à–µ–Ω–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥
                    video_element = message.find('video')
                    if video_element:
                        video_url = video_element.get('src')
                        poster_url = video_element.get('poster')
                        
                        # –ï—Å–ª–∏ –Ω–µ—Ç poster, –ø—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ thumbnail
                        if not poster_url:
                            poster_element = video_element.find('img')
                            if poster_element:
                                poster_url = poster_element.get('src')
                        
                        if video_url:
                            if video_url.startswith('//'):
                                video_url = 'https:' + video_url
                            if poster_url and poster_url.startswith('//'):
                                poster_url = 'https:' + poster_url
                            
                            media = {
                                'type': 'video',
                                'url': video_url,
                                'thumbnail': poster_url,
                                'width': None,
                                'height': None
                            }
                            logger.info(f"Found video: {video_url}")
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç—ã/—Ñ–∞–π–ª—ã
                    document_wrap = message.find('a', class_='tgme_widget_message_document_wrap')
                    if document_wrap and not media:
                        doc_icon = document_wrap.find('i', class_='tgme_widget_message_document_icon')
                        if doc_icon:
                            media = {
                                'type': 'document',
                                'url': None,
                                'thumbnail': None,
                                'width': None,
                                'height': None
                            }
                    
                    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ –∏–∑ –ø–µ—Ä–≤—ã—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
                    sentences = re.split(r'[.!?]+', full_text)
                    title = sentences[0][:150].strip() if sentences and sentences[0] else f"–ü–æ—Å—Ç –æ—Ç {channel_data['name']}"
                    
                    # –û—Ü–µ–Ω–∫–∞ –≤—Ä–µ–º–µ–Ω–∏ —á—Ç–µ–Ω–∏—è (200 —Å–ª–æ–≤ –≤ –º–∏–Ω—É—Ç—É)
                    word_count = len(full_text.split())
                    reading_time = max(1, word_count // 200)
                    
                    post = {
                        'id': hashlib.md5(f"{channel_data['username']}_{i}_{full_text[:50]}".encode()).hexdigest(),
                        'title': title,
                        'text': full_text,  # –ü–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç –±–µ–∑ —Å–æ–∫—Ä–∞—â–µ–Ω–∏–π
                        'content_html': html_content,  # HTML –∫–æ–Ω—Ç–µ–Ω—Ç –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
                        'link': f"https://t.me/{channel_data['username']}",
                        'date': date,
                        'source': channel_data['name'],
                        'category': channel_data['category'],
                        'channel': channel_data['username'],
                        'media': media,
                        'reading_time': reading_time,
                        'word_count': word_count
                    }
                    
                    posts.append(post)
                    
                except Exception as e:
                    logger.warning(f"Error parsing message {i}: {e}")
                    continue
                    
        except Exception as e:
            logger.error(f"Error parsing HTML content: {e}")
        
        if not posts:  # –ï—Å–ª–∏ –ø–∞—Ä—Å–∏–Ω–≥ –Ω–µ —É–¥–∞–ª—Å—è, –∏—Å–ø–æ–ª—å–∑—É–µ–º –º–æ–∫ –¥–∞–Ω–Ω—ã–µ
            return self._generate_mock_posts(channel_data)
        
        return posts

    def _generate_mock_posts(self, channel_data: Dict) -> List[Dict[str, Any]]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –º–æ–∫ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∫–∞–Ω–∞–ª–∞ —Å–æ–≥–ª–∞—Å–Ω–æ –¢–ó"""
        posts = []
        base_time = datetime.now()

        # –ö–æ–Ω—Ç–µ–Ω—Ç –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –∫–∞–Ω–∞–ª–∞
        content_templates = {
            'gifts': [
                "üéÅ –ù–æ–≤—ã–µ –±–µ—Å–ø–ª–∞—Ç–Ω—ã–µ –ø–æ–¥–∞—Ä–∫–∏! –£—Å–ø–µ–π—Ç–µ –ø–æ–ª—É—á–∏—Ç—å —ç–∫—Å–∫–ª—é–∑–∏–≤–Ω—ã–µ –±–æ–Ω—É—Å—ã",
                "üíù –ü—Ä–æ–º–æ–∫–æ–¥—ã –Ω–∞ —Å–∫–∏–¥–∫–∏ –¥–æ 70%! –û–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ",
                "üéâ –†–æ–∑—ã–≥—Ä—ã—à —Ü–µ–Ω–Ω—ã—Ö –ø—Ä–∏–∑–æ–≤ —Å—Ä–µ–¥–∏ –ø–æ–¥–ø–∏—Å—á–∏–∫–æ–≤ –∫–∞–Ω–∞–ª–∞",
                "üõçÔ∏è –õ—É—á—à–∏–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –¥–Ω—è - –Ω–µ –ø—Ä–æ–ø—É—Å—Ç–∏—Ç–µ!"
            ],
            'crypto': [
                "üìà –ê–Ω–∞–ª–∏–∑ —Ä—ã–Ω–∫–∞: Bitcoin –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Ä–æ—Å—Ç –Ω–∞ 5%",
                "üí∞ –ù–æ–≤—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ DeFi –∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–π - –æ–±–∑–æ—Ä –ø—Ä–æ–µ–∫—Ç–æ–≤",
                "üöÄ –ü–µ—Ä—Å–ø–µ–∫—Ç–∏–≤–Ω—ã–µ –∞–ª—å—Ç–∫–æ–∏–Ω—ã –¥–ª—è –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω—ã—Ö –∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–π",
                "‚ö° –°—Ä–æ—á–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏: –∫—Ä—É–ø–Ω—ã–µ –¥–≤–∏–∂–µ–Ω–∏—è –Ω–∞ –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç–Ω–æ–º —Ä—ã–Ω–∫–µ"
            ],
            'nft': [
                "üñºÔ∏è –ù–æ–≤–∞—è –∫–æ–ª–ª–µ–∫—Ü–∏—è NFT –æ—Ç –∏–∑–≤–µ—Å—Ç–Ω–æ–≥–æ —Ö—É–¥–æ–∂–Ω–∏–∫–∞ —É–∂–µ –≤ –ø—Ä–æ–¥–∞–∂–µ",
                "üíé –†–∞—Ä–∏—Ç–µ—Ç–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã –Ω–∞ –∞—É–∫—Ü–∏–æ–Ω–µ - –ø–æ—Å–ª–µ–¥–Ω–∏–π —à–∞–Ω—Å –ø—Ä–∏–æ–±—Ä–µ—Å—Ç–∏",
                "üé® –û–±–∑–æ—Ä –ª—É—á—à–∏—Ö NFT —Ö—É–¥–æ–∂–Ω–∏–∫–æ–≤ –Ω–µ–¥–µ–ª–∏",
                "üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ NFT —Ä—ã–Ω–∫–∞: —Ä–æ—Å—Ç –æ–±—ä–µ–º–æ–≤ —Ç–æ—Ä–≥–æ–≤ –Ω–∞ 15%"
            ],
            'tech': [
                "üíª –†–µ–≤–æ–ª—é—Ü–∏–æ–Ω–Ω—ã–µ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏ 2025 –≥–æ–¥–∞ - —á—Ç–æ –Ω–∞—Å –∂–¥–µ—Ç",
                "üîß –û–±–∑–æ—Ä –Ω–æ–≤–µ–π—à–∏—Ö –≥–∞–¥–∂–µ—Ç–æ–≤ –æ—Ç –º–∏—Ä–æ–≤—ã—Ö –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª–µ–π",
                "üöÄ –°—Ç–∞—Ä—Ç–∞–ø—ã –≤ —Å—Ñ–µ—Ä–µ –ò–ò –ø—Ä–∏–≤–ª–µ–∫–ª–∏ —Ä–µ–∫–æ—Ä–¥–Ω—ã–µ –∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–∏",
                "üì± –¢–û–ü –º–æ–±–∏–ª—å–Ω—ã—Ö –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–π –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏"
            ],
            'community': [
                "üë• –û–±—Å—É–∂–¥–µ–Ω–∏–µ –∞–∫—Ç—É–∞–ª—å–Ω—ã—Ö —Ç–µ–º –≤ –Ω–∞—à–µ–º —Å–æ–æ–±—â–µ—Å—Ç–≤–µ",
                "üí¨ –í–∞–∂–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏ –∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –¥–ª—è —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤",
                "üîî –ê–Ω–æ–Ω—Å –ø—Ä–µ–¥—Å—Ç–æ—è—â–∏—Ö –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏–π –∏ –≤—Å—Ç—Ä–µ—á",
                "üì¢ –ü–æ–ª–µ–∑–Ω—ã–µ —Å–æ–≤–µ—Ç—ã –∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –æ—Ç —ç–∫—Å–ø–µ—Ä—Ç–æ–≤"
            ]
        }

        templates = content_templates.get(channel_data['category'], content_templates['community'])

        for i in range(5):  # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º 5 –ø–æ—Å—Ç–æ–≤
            post_time = base_time - timedelta(hours=i * 4 + hash(channel_data['username']) % 12)

            text = templates[i % len(templates)]
            title = text.split('.')[0][:80] + ("..." if len(text.split('.')[0]) > 80 else "")

            posts.append({
                'id': hashlib.md5(f"{channel_data['username']}_{i}_{text}".encode()).hexdigest(),
                'title': title,
                'text': text,
                'link': f"https://t.me/{channel_data['username']}",
                'date': post_time.isoformat(),
                'source': channel_data['name'],
                'category': channel_data['category'],
                'channel': channel_data['username']
            })

        return posts
        """–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º"""
        content = (title + " " + description).lower()

        for category, keywords in self.keywords.items():
            if any(keyword in content for keyword in keywords):
                return category

        return 'general'

    async def fetch_rss_feed(self, source: Dict[str, str]) -> List[Dict[str, Any]]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–µ–π –∏–∑ RSS –∏—Å—Ç–æ—á–Ω–∏–∫–∞"""
        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º feedparser –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ RSS
            feed = feedparser.parse(source['url'])

            if not feed.entries:
                logger.warning(f"No entries found in RSS feed: {source['url']}")
                return []

            articles = []
            for entry in feed.entries[:10]:  # –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 10 –Ω–æ–≤–æ—Å—Ç–µ–π
                # –ü–æ–ª—É—á–∞–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ –∏–∑ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –ø–æ–ª–µ–π
                description = ""
                if hasattr(entry, 'summary'):
                    description = entry.summary
                elif hasattr(entry, 'description'):
                    description = entry.description
                elif hasattr(entry, 'content'):
                    description = entry.content[0].value if entry.content else ""

                # –û—á–∏—â–∞–µ–º HTML —Ç–µ–≥–∏
                clean_description = re.sub(r'<[^>]+>', '', description)
                clean_description = clean_description[:200] + "..." if len(
                    clean_description) > 200 else clean_description

                # –ü–æ–ª—É—á–∞–µ–º –¥–∞—Ç—É –ø—É–±–ª–∏–∫–∞—Ü–∏–∏
                pub_date = datetime.now()
                if hasattr(entry, 'published_parsed') and entry.published_parsed:
                    import time
                    pub_date = datetime.fromtimestamp(time.mktime(entry.published_parsed))
                elif hasattr(entry, 'updated_parsed') and entry.updated_parsed:
                    import time
                    pub_date = datetime.fromtimestamp(time.mktime(entry.updated_parsed))

                # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏—è
                auto_category = self.categorize_content(entry.title, clean_description)
                final_category = source.get('category', auto_category)

                # –ò–∑–≤–ª–µ–∫–∞–µ–º –º–µ–¥–∏–∞ –∫–æ–Ω—Ç–µ–Ω—Ç
                media_list = []

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º enclosures (–≤–ª–æ–∂–µ–Ω–∏—è)
                if hasattr(entry, 'enclosures') and entry.enclosures:
                    for enclosure in entry.enclosures:
                        if hasattr(enclosure, 'type'):
                            if enclosure.type.startswith('image/'):
                                media_list.append({
                                    'type': 'photo',
                                    'url': enclosure.href,
                                    'thumbnail': enclosure.href
                                })
                            elif enclosure.type.startswith('video/'):
                                media_list.append({
                                    'type': 'video',
                                    'url': enclosure.href,
                                    'thumbnail': None
                                })

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º media:content (–∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π —Å–ø–æ—Å–æ–±)
                if hasattr(entry, 'media_content') and entry.media_content:
                    for media_item in entry.media_content:
                        if media_item.get('type', '').startswith('image/'):
                            media_list.append({
                                'type': 'photo',
                                'url': media_item.get('url', ''),
                                'thumbnail': media_item.get('url', '')
                            })

                # –§–æ—Ä–º–∏—Ä—É–µ–º HTML –∫–æ–Ω—Ç–µ–Ω—Ç
                content_html = clean_description
                for media in media_list:
                    if media.get('type') == 'photo' and media.get('url'):
                        content_html += f'<br><img src="{media["url"]}" style="max-width:100%; height:auto; border-radius:8px; margin:10px 0;"/>'
                    elif media.get('type') == 'video' and media.get('url'):
                        thumbnail = media.get('thumbnail', '')
                        content_html += f'<br><video controls poster="{thumbnail}" style="max-width:100%; height:auto; border-radius:8px; margin:10px 0;">'
                        content_html += f'<source src="{media["url"]}" type="video/mp4">'
                        content_html += '</video>'

                article = {
                    'id': hashlib.md5((entry.link + entry.title).encode()).hexdigest(),
                    'title': entry.title,
                    'text': clean_description,  # Plain text
                    'content_html': content_html,  # HTML —Å –º–µ–¥–∏–∞
                    'link': entry.link,
                    'date': pub_date.isoformat(),
                    'source': source['name'],
                    'category': final_category,
                    'channel': 'rss_' + source['name'].lower().replace(' ', '_'),
                    'media': media_list
                }

                articles.append(article)

            return articles

        except Exception as e:
            logger.error(f"Error fetching RSS feed {source['url']}: {e}")
            return []
        """–ü–æ–ª—É—á–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∫–∞–Ω–∞–ª–µ —á–µ—Ä–µ–∑ Telegram API"""
        try:
            # –í —Ä–µ–∞–ª—å–Ω–æ–π —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ –∑–¥–µ—Å—å –±—É–¥–µ—Ç –≤—ã–∑–æ–≤ –∫ Telegram Bot API
            # –ü–æ–∫–∞ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –º–æ–∫ –¥–∞–Ω–Ω—ã–µ
            channel_data = next((ch for ch in self.channels if ch['username'] == username), None)
            if not channel_data:
                return None

            return {
                'username': username,
                'title': channel_data['name'],
                'description': f"–ö–∞–Ω–∞–ª {channel_data['name']} - {channel_data['category']}",
                'subscribers_count': 1000 + hash(username) % 50000,  # –ú–æ–∫ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –ø–æ–¥–ø–∏—Å—á–∏–∫–æ–≤
                'category': channel_data['category']
            }
        except Exception as e:
            logger.error(f"Error getting channel info for {username}: {e}")
            return None

    async def get_channel_posts(self, username: str, limit: int = 10) -> List[Dict[str, Any]]:
        """–ü–æ–ª—É—á–∏—Ç—å –ø–æ—Å–ª–µ–¥–Ω–∏–µ –ø–æ—Å—Ç—ã –∏–∑ –∫–∞–Ω–∞–ª–∞"""
        try:
            # –í —Ä–µ–∞–ª—å–Ω–æ–π —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ –∑–¥–µ—Å—å –±—É–¥–µ—Ç –≤—ã–∑–æ–≤ –∫ Telegram Bot API
            # –ü–æ–∫–∞ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º –º–æ–∫ –¥–∞–Ω–Ω—ã–µ
            channel_data = next((ch for ch in self.channels if ch['username'] == username), None)
            if not channel_data:
                return []

            posts = []
            base_time = datetime.now()

            for i in range(limit):
                post_time = base_time - timedelta(hours=i * 2 + hash(username + str(i)) % 24)

                # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
                if channel_data['category'] == 'gifts':
                    titles = [
                        f"üéÅ –ù–æ–≤—ã–µ –±–µ—Å–ø–ª–∞—Ç–Ω—ã–µ –ø–æ–¥–∞—Ä–∫–∏ –≤ {channel_data['name']}!",
                        f"üíù –≠–∫—Å–∫–ª—é–∑–∏–≤–Ω—ã–µ –ø—Ä–æ–º–æ–∫–æ–¥—ã –∏ —Å–∫–∏–¥–∫–∏",
                        f"üéâ –†–æ–∑—ã–≥—Ä—ã—à –ø—Ä–∏–∑–æ–≤ –¥–ª—è –ø–æ–¥–ø–∏—Å—á–∏–∫–æ–≤",
                        f"üõçÔ∏è –õ—É—á—à–∏–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –¥–Ω—è"
                    ]
                elif channel_data['category'] == 'crypto':
                    titles = [
                        f"üìà –ê–Ω–∞–ª–∏–∑ —Ä—ã–Ω–∫–∞ –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç",
                        f"üí∞ –ù–æ–≤—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –¥–ª—è –∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–π",
                        f"üöÄ –û–±–∑–æ—Ä –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤–Ω—ã—Ö –ø—Ä–æ–µ–∫—Ç–æ–≤",
                        f"‚ö° –ë—ã—Å—Ç—Ä—ã–µ –Ω–æ–≤–æ—Å—Ç–∏ –∏–∑ –º–∏—Ä–∞ –∫—Ä–∏–ø—Ç–æ"
                    ]
                elif channel_data['category'] == 'nft':
                    titles = [
                        f"üñºÔ∏è –ù–æ–≤—ã–µ NFT –∫–æ–ª–ª–µ–∫—Ü–∏–∏",
                        f"üíé –†–∞—Ä–∏—Ç–µ—Ç–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã –Ω–∞ –∞—É–∫—Ü–∏–æ–Ω–µ",
                        f"üé® –û–±–∑–æ—Ä NFT —Ö—É–¥–æ–∂–Ω–∏–∫–æ–≤",
                        f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ NFT —Ä—ã–Ω–∫–∞"
                    ]
                elif channel_data['category'] == 'tech':
                    titles = [
                        f"üíª –ù–æ–≤–æ—Å—Ç–∏ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–π",
                        f"üîß –û–±–∑–æ—Ä –≥–∞–¥–∂–µ—Ç–æ–≤",
                        f"üöÄ –ò–Ω–Ω–æ–≤–∞—Ü–∏–∏ –≤ IT",
                        f"üì± –ú–æ–±–∏–ª—å–Ω—ã–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è"
                    ]
                else:
                    titles = [
                        f"üì¢ –ù–æ–≤–æ—Å—Ç–∏ –æ—Ç {channel_data['name']}",
                        f"‚ÑπÔ∏è –í–∞–∂–Ω—ã–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è",
                        f"üìù –ü–æ–ª–µ–∑–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è",
                        f"üî• –ì–æ—Ä—è—á–∏–µ —Ç–µ–º—ã"
                    ]

                title = titles[i % len(titles)]

                posts.append({
                    'id': f"{username}_{i}",
                    'title': title,
                    'text': f"–ò–Ω—Ç–µ—Ä–µ—Å–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç –æ—Ç –∫–∞–Ω–∞–ª–∞ {channel_data['name']}. –ü–æ–¥–ø–∏—Å—ã–≤–∞–π—Ç–µ—Å—å –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –∞–∫—Ç—É–∞–ª—å–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π!",
                    'date': post_time.isoformat(),
                    'views': 100 + hash(username + str(i)) % 5000,
                    'link': f"https://t.me/{username}",
                    'channel': username,
                    'category': channel_data['category']
                })

            return posts

        except Exception as e:
            logger.error(f"Error getting posts for {username}: {e}")
            return []

    async def get_all_news(self, category: str = 'all', limit: int = 50) -> List[Dict[str, Any]]:
        """
        –ü–æ–ª—É—á–∏—Ç—å –Ω–æ–≤–æ—Å—Ç–∏ –∏–∑ –≤—Å–µ—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ —Å–æ–≥–ª–∞—Å–Ω–æ –¢–ó:
        - Telegram –∫–∞–Ω–∞–ª—ã (–æ—Å–Ω–æ–≤–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏)
        - RSS –ª–µ–Ω—Ç—ã (–¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏)
        - –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏—è –∏ –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è
        """
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à
            cache_key = f"news_{category}_{limit}"
            if cache_key in self.cache:
                cached_data, cached_time = self.cache[cache_key]
                if datetime.now() - cached_time < self.cache_ttl:
                    logger.info(f"Returning cached news for {category}, {len(cached_data)} items")
                    return cached_data

            all_posts = []

            # 1. –ü–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ Telegram –∫–∞–Ω–∞–ª–æ–≤ (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫ —Å–æ–≥–ª–∞—Å–Ω–æ –¢–ó)
            telegram_channels = self.channels
            if category != 'all':
                telegram_channels = [ch for ch in self.channels if ch['category'] == category]

            logger.info(f"Fetching from {len(telegram_channels)} Telegram channels")

            # –ü–æ–ª—É—á–∞–µ–º –ø–æ—Å—Ç—ã –∏–∑ Telegram –∫–∞–Ω–∞–ª–æ–≤
            telegram_tasks = []
            for channel in telegram_channels:
                telegram_tasks.append(self.fetch_telegram_channel(channel['username']))

            telegram_results = await asyncio.gather(*telegram_tasks, return_exceptions=True)

            for i, result in enumerate(telegram_results):
                if isinstance(result, list):
                    all_posts.extend(result)
                    logger.info(f"Got {len(result)} posts from {telegram_channels[i]['username']}")
                else:
                    logger.error(f"Error fetching posts from {telegram_channels[i]['username']}: {result}")

            # 2. –ü–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ RSS –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ (–¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫)
            rss_sources = self.rss_sources
            if category != 'all':
                rss_sources = [src for src in self.rss_sources if src['category'] == category]

            logger.info(f"Fetching from {len(rss_sources)} RSS sources")

            # –ü–æ–ª—É—á–∞–µ–º —Å—Ç–∞—Ç—å–∏ –∏–∑ RSS
            rss_tasks = []
            for source in rss_sources:
                rss_tasks.append(self.fetch_rss_feed(source))

            rss_results = await asyncio.gather(*rss_tasks, return_exceptions=True)

            for i, result in enumerate(rss_results):
                if isinstance(result, list):
                    all_posts.extend(result)
                    logger.info(f"Got {len(result)} articles from {rss_sources[i]['name']}")
                else:
                    logger.error(f"Error fetching RSS from {rss_sources[i]['url']}: {result}")

            # 3. –û–±—Ä–∞–±–æ—Ç–∫–∞ –∏ –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è —Å–æ–≥–ª–∞—Å–Ω–æ –¢–ó
            # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ –∑–∞–≥–æ–ª–æ–≤–∫—É –∏ —Å—Å—ã–ª–∫–µ
            seen = set()
            unique_posts = []
            for post in all_posts:
                # –°–æ–∑–¥–∞–µ–º –∫–ª—é—á –¥–ª—è –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏
                title_clean = re.sub(r'[^\w\s]', '', post['title'].lower()).strip()
                key = (title_clean, post.get('link', ''))
                if key not in seen:
                    seen.add(key)
                    unique_posts.append(post)

            logger.info(f"After deduplication: {len(unique_posts)} unique posts from {len(all_posts)} total")

            # 4. –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –¥–∞—Ç–µ (–Ω–æ–≤—ã–µ —Å–Ω–∞—á–∞–ª–∞)
            try:
                unique_posts.sort(
                    key=lambda x: datetime.fromisoformat(x['date'].replace('Z', '+00:00')),
                    reverse=True
                )
            except Exception as e:
                logger.warning(f"Error sorting by date: {e}, using original order")

            # 5. –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ–≥–ª–∞—Å–Ω–æ –¢–ó
            final_posts = unique_posts[:limit]

            # 6. –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫—ç—à –Ω–∞ 30 –º–∏–Ω—É—Ç —Å–æ–≥–ª–∞—Å–Ω–æ –¢–ó
            self.cache[cache_key] = (final_posts, datetime.now())

            logger.info(f"Returning {len(final_posts)} news items for category '{category}'")
            return final_posts

        except Exception as e:
            logger.error(f"Error in get_all_news: {e}")
            # –í —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ –∫—ç—à–∞, –µ—Å–ª–∏ –µ—Å—Ç—å
            cache_key = f"news_{category}_{limit}"
            if cache_key in self.cache:
                cached_data, _ = self.cache[cache_key]
                logger.info("Returning stale cached data due to error")
                return cached_data
            return []

            return final_posts

        except Exception as e:
            logger.error(f"Error getting all news: {e}")
            return []

    async def get_channels_info(self) -> List[Dict[str, Any]]:
        """–ü–æ–ª—É—á–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –≤—Å–µ—Ö –∫–∞–Ω–∞–ª–∞—Ö"""
        try:
            tasks = []
            for channel in self.channels:
                tasks.append(self.get_channel_info(channel['username']))

            results = await asyncio.gather(*tasks, return_exceptions=True)

            channels_info = []
            for result in results:
                if isinstance(result, dict):
                    channels_info.append(result)

            return channels_info

        except Exception as e:
            logger.error(f"Error getting channels info: {e}")
            return []

    async def update_news_async(self):
        """
        –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–µ–π –∏–∑ –≤—Å–µ—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
        –ú–µ—Ç–æ–¥ –¥–ª—è –ø–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–æ–≥–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –≤ main.py
        """
        try:
            logger.info(f"Fetching from {len(self.channels)} Telegram channels")

            # –ü–æ–ª—É—á–∞–µ–º –Ω–æ–≤–æ—Å—Ç–∏ –∏–∑ Telegram –∫–∞–Ω–∞–ª–æ–≤
            all_posts = []
            for channel in self.channels:
                try:
                    posts = await self.fetch_telegram_channel(channel['username'])
                    logger.info(f"Got {len(posts)} posts from {channel['username']}")
                    all_posts.extend(posts)
                except Exception as e:
                    logger.error(f"Error fetching from channel {channel['username']}: {e}")
                    continue

            # –ü–æ–ª—É—á–∞–µ–º –Ω–æ–≤–æ—Å—Ç–∏ –∏–∑ RSS –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
            logger.info(f"Fetching from {len(self.rss_sources)} RSS sources")
            for source in self.rss_sources:
                try:
                    articles = await self.fetch_rss_source(source['url'], source['name'], source['category'])
                    logger.info(f"Got {len(articles)} articles from {source['name']}")
                    all_posts.extend(articles)
                except Exception as e:
                    logger.error(f"Error fetching from RSS {source['name']}: {e}")
                    continue

            # –î–µ–¥—É–ø–ª–∏—Ü–∏—Ä—É–µ–º –ø–æ –∑–∞–≥–æ–ª–æ–≤–∫–∞–º
            unique_posts = []
            seen_titles = set()

            for post in all_posts:
                title_hash = hashlib.md5(post['title'].encode()).hexdigest()
                if title_hash not in seen_titles:
                    seen_titles.add(title_hash)
                    unique_posts.append(post)

            logger.info(f"After deduplication: {len(unique_posts)} unique posts from {len(all_posts)} total")

            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –¥–∞—Ç–µ (–Ω–æ–≤—ã–µ —Å–Ω–∞—á–∞–ª–∞)
            try:
                unique_posts.sort(key=lambda x: x['date'], reverse=True)
            except Exception as e:
                logger.warning(f"Error sorting by date: {e}, using original order")

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö
            await self.save_to_database(unique_posts[:50])  # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–æ–ª—å–∫–æ 50 —Å–∞–º—ã—Ö —Å–≤–µ–∂–∏—Ö

            logger.info(f"Successfully updated {len(unique_posts[:50])} news items")

        except Exception as e:
            logger.error(f"Error in update_news_async: {e}")
            raise

    async def fetch_rss_source(self, url: str, name: str, category: str) -> List[Dict[str, Any]]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–µ–π –∏–∑ RSS –∏—Å—Ç–æ—á–Ω–∏–∫–∞"""
        try:
            async with aiohttp.ClientSession() as session:
                async with session.get(url, timeout=10) as response:
                    if response.status != 200:
                        logger.warning(f"Failed to fetch RSS {url}, status: {response.status}")
                        return []

                    content = await response.text()
                    feed = feedparser.parse(content)

                    if not feed.entries:
                        logger.warning(f"No entries found in RSS feed: {url}")
                        return []

                    articles = []
                    for entry in feed.entries[:10]:  # –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ 10 –ø–æ—Å–ª–µ–¥–Ω–∏—Ö —Å—Ç–∞—Ç–µ–π
                        # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ—Å–Ω–æ–≤–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
                        title = entry.get('title', '–ë–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞')
                        description = entry.get('description', '') or entry.get('summary', '')
                        link = entry.get('link', '')

                        # –ü–∞—Ä—Å–∏–º –¥–∞—Ç—É
                        date = datetime.now().isoformat()
                        if hasattr(entry, 'published_parsed') and entry.published_parsed:
                            try:
                                import time
                                date = datetime.fromtimestamp(time.mktime(entry.published_parsed)).isoformat()
                            except:
                                pass

                        # –ò–∑–≤–ª–µ–∫–∞–µ–º –º–µ–¥–∏–∞ –∫–æ–Ω—Ç–µ–Ω—Ç
                        media = None

                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º enclosures (–≤–ª–æ–∂–µ–Ω–∏—è)
                        if hasattr(entry, 'enclosures') and entry.enclosures:
                            for enclosure in entry.enclosures:
                                if hasattr(enclosure, 'type'):
                                    if enclosure.type.startswith('image/'):
                                        media = {
                                            'type': 'photo',
                                            'url': enclosure.href,
                                            'thumbnail': enclosure.href
                                        }
                                        break
                                    elif enclosure.type.startswith('video/'):
                                        media = {
                                            'type': 'video',
                                            'url': enclosure.href,
                                            'thumbnail': None
                                        }
                                        break

                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º media:content (–∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π —Å–ø–æ—Å–æ–±)
                        if not media and hasattr(entry, 'media_content') and entry.media_content:
                            for media_item in entry.media_content:
                                if media_item.get('type', '').startswith('image/'):
                                    media = {
                                        'type': 'photo',
                                        'url': media_item.get('url', ''),
                                        'thumbnail': media_item.get('url', '')
                                    }
                                    break

                        # –û—á–∏—â–∞–µ–º HTML —Ç–µ–≥–∏ –∏–∑ –æ–ø–∏—Å–∞–Ω–∏—è
                        import re
                        clean_description = re.sub(r'<[^>]+>', '', description)
                        clean_description = clean_description.strip()[:300] + "..." if len(clean_description) > 300 else clean_description.strip()

                        article = {
                            'id': hashlib.md5(f"{url}_{title}".encode()).hexdigest(),
                            'title': title,
                            'text': clean_description,
                            'link': link,
                            'date': date,
                            'source': name,
                            'category': category,
                            'media': media
                        }

                        articles.append(article)

                    return articles

        except Exception as e:
            logger.error(f"Error fetching RSS from {url}: {e}")
            return []

    async def save_to_database(self, posts: List[Dict[str, Any]]):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–µ–π –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö"""
        try:
            # –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –∑–¥–µ—Å—å, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å —Ü–∏—Ä–∫—É–ª—è—Ä–Ω–æ–≥–æ –∏–º–ø–æ—Ä—Ç–∞
            import server.main as main_module
            from server.db import NewsItem

            if main_module.SessionLocal is None:
                logger.error("Database not initialized")
                return

            db = main_module.SessionLocal()

            try:
                news_items = []
                for post in posts:
                    # –ü–æ–ª—É—á–∞–µ–º –∏–ª–∏ —Å–æ–∑–¥–∞—ë–º –∏—Å—Ç–æ—á–Ω–∏–∫
                    from server.services.news_service import get_or_create_source
                    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –∏ url
                    source_type = 'telegram' if post.get('channel') and not post.get('link', '').startswith('http') else 'rss'
                    source_url = post.get('link') or ''
                    category = post.get('category') or 'general'
                    source = get_or_create_source(db, post.get('source', 'unknown'), url=source_url, source_type=source_type, category=category)
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ —É–∂–µ —Ç–∞–∫–∞—è –Ω–æ–≤–æ—Å—Ç—å
                    existing = db.query(NewsItem).filter(
                        NewsItem.title == post['title']
                    ).first()

                    if existing:
                        continue  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã

                    # –ò–∑–≤–ª–µ–∫–∞–µ–º –º–µ–¥–∏–∞ –¥–∞–Ω–Ω—ã–µ
                    image_url = None
                    video_url = None
                    media_json = None

                    if post.get('media'):
                        media_json = post['media']  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ–ª–Ω—ã–π JSON –º–µ–¥–∏–∞
                        if post['media']['type'] == 'photo':
                            image_url = post['media']['url']
                        elif post['media']['type'] == 'video':
                            video_url = post['media']['url']
                            if not image_url and post['media'].get('thumbnail'):
                                image_url = post['media']['thumbnail']

                    # –ü–æ–ª—É—á–∞–µ–º HTML –∫–æ–Ω—Ç–µ–Ω—Ç –µ—Å–ª–∏ –µ—Å—Ç—å
                    content_html = post.get('content_html', '')
                    
                    # –û—Ü–µ–Ω–∫–∞ –≤—Ä–µ–º–µ–Ω–∏ —á—Ç–µ–Ω–∏—è (–∏—Å–ø–æ–ª—å–∑—É–µ–º —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –∏–ª–∏ –≤—ã—á–∏—Å–ª—è–µ–º)
                    reading_time = post.get('reading_time', 1)
                    if not reading_time:
                        word_count = len(post['text'].split()) if post.get('text') else 0
                        reading_time = max(1, word_count // 200)

                    news_item = NewsItem(
                        source_id=source.id,
                        title=post['title'],
                        content=post['text'],
                        content_html=content_html,  # –°–æ—Ö—Ä–∞–Ω—è–µ–º HTML –∫–æ–Ω—Ç–µ–Ω—Ç
                        link=post['link'],
                        publish_date=datetime.fromisoformat(post['date'].replace('Z', '+00:00')),
                        category=category,
                        media=media_json,  # –°–æ—Ö—Ä–∞–Ω—è–µ–º JSON –º–µ–¥–∏–∞
                        image_url=image_url,
                        video_url=video_url,
                        reading_time=reading_time,
                        views_count=0,
                        author=post.get('source'),
                        subtitle=None
                    )

                    news_items.append(news_item)

                if news_items:
                    db.bulk_save_objects(news_items)
                    db.commit()
                    logger.info(f"Saved {len(news_items)} new items to database")
                else:
                    logger.info("No new items to save")

            finally:
                db.close()

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –∫–∞–Ω–∞–ª–æ–≤: {e}")
            if 'db' in locals():
                db.rollback()
                db.close()
